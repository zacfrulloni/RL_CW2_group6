{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxThEpkQZram",
        "outputId": "2f56f4c8-61c0-40dd-b59c-fefa8159fdb7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Requirement already satisfied: gym[box2d] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (0.0.8)\n",
            "Collecting box2d-py==2.3.5 (from gym[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pygame==2.1.0 (from gym[box2d])\n",
            "  Downloading pygame-2.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gym[box2d]) (4.2.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376134 sha256=efc930ef8846b743cb2e2c42069f4c9fec7252bdcfc627fbf3e374f50fa3ced6\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py, pygame\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "Successfully installed box2d-py-2.3.5 pygame-2.1.0\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n",
            "Collecting ffmpeg\n",
            "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: ffmpeg\n",
            "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6082 sha256=bc53cb5581b12d788468d66c7025b3dc11b44ff5a2cd77b0155de9c8bc5c6846\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/7a/69/cd6aeb83b126a7f04cbe7c9d929028dc52a6e7d525ff56003a\n",
            "Successfully built ffmpeg\n",
            "Installing collected packages: ffmpeg\n",
            "Successfully installed ffmpeg-1.4\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from moviepy) (1.25.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy) (0.4.9)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install swig\n",
        "!pip install gym[box2d]\n",
        "!pip install moviepy\n",
        "!pip install ffmpeg --upgrade\n",
        "!pip install moviepy --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "W7j4EQM4Zrar",
        "outputId": "38aae3a1-5247-4508-badf-0d2401e385f6"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-b01046ab73e3>\u001b[0m in \u001b[0;36m<cell line: 410>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;31m# Train the agent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madditional_graphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b01046ab73e3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m                 \u001b[0;31m# optimize the policy wiht Huber loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhuber_loss_optimize_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 \u001b[0;31m# Move to the next state to continue the learning process\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b01046ab73e3>\u001b[0m in \u001b[0;36mhuber_loss_optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;31m#get a sample of the memory so that a set of transitions can be further manipulated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtransitions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m         \u001b[0;31m#get the data ready for data manipulation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtransitions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-b01046ab73e3>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m#determine the probability any given priority will be selected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpriorities\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpriorities\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0;31m#randomly select a bunch of positions within the memory that is equalivant of the size of the batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math, random\n",
        "from itertools import count\n",
        "from collections import namedtuple, deque\n",
        "import matplotlib\n",
        "from IPython import display\n",
        "from gym.wrappers import RecordVideo\n",
        "\n",
        "\n",
        "#You can use seed 1 for a nice result\n",
        "#seed = 1\n",
        "#random.seed(seed)\n",
        "#np.random.seed(seed)\n",
        "\n",
        "# define the Lunar Lander environment\n",
        "tmp_env = gym.make('LunarLander-v2', render_mode='rgb_array')\n",
        "env = gym.wrappers.RecordVideo(env = tmp_env, video_folder=\"/Users/Admin/Desktop\", video_length = 0, name_prefix=\"lunar-agent-video\", episode_trigger=lambda eps: eps % 50 == 0)\n",
        "\n",
        "\n",
        "# check if running in an IPython environment\n",
        "is_ipython = 'inline' in matplotlib.get_backend()\n",
        "\n",
        "# set device, dictated by the availability of NVIDIA CUDA\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Define the transition tuple\n",
        "Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "\n",
        "\n",
        "class DuelingDQN(nn.Module):\n",
        "    \"\"\"\n",
        "        Source for Dueling DQN\n",
        "        Curt-Park (2024) Notebook on nbviewer, Jupyter Notebook Viewer. Available at: https://nbviewer.org/github/Curt-Park/rainbow-is-all-you-need/blob/master/04.dueling.ipynb (Accessed: 04 May 2024).\n",
        "\n",
        "        Source for Double DQN\n",
        "        Curt-Park (2024) Double DQN, GitHub. Available at: https://github.com/Curt-Park/rainbow-is-all-you-need/blob/master/02.double_q.ipynb (Accessed: 04 May 2024).\n",
        "    \"\"\"\n",
        "    def __init__(self, n_observations, n_actions):\n",
        "\n",
        "        super(DuelingDQN, self).__init__()\n",
        "         # Defines the first Linear layer of the network.\n",
        "        # It takes `n_observations` as input size (the dimensionality of the state space)\n",
        "        # and outputs 256 features to the next layer.\n",
        "        self.layer1 = nn.Linear(n_observations, 256)\n",
        "\n",
        "        # The second Linear layer takes 256 input features (from layer1) and\n",
        "        # also outputs 128 features for consistency and depth of the network.\n",
        "        self.layer2 = nn.Linear(256, 128)\n",
        "\n",
        "        # The third Linear layer takes the 128 input features (from layer2) and outputs `n_actions` features, where each feature corresponds to the\n",
        "        # value (Q-value) of each possible action given the input state to the network.\n",
        "        # This approximates the action-value function Q(s, a).\n",
        "        #this takes the prior layer and aims to esitmate and find the single value for the value function of a state\n",
        "        self.value_head = nn.Linear(128, 1)\n",
        "        #this aims to take the same information from the prior layer and find the value of the advantage function\n",
        "        self.advantage_head = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, state):\n",
        "         # Applies a ReLU (Rectified Linear Unit) activation function to the output of the first linear layer. This adds non-linearity to the model,\n",
        "        #  helping it to learn more complex functions.\n",
        "        state = F.relu(self.layer1(state))\n",
        "\n",
        "        # Again, applies a ReLU activation function to the output\n",
        "        # of the second linear layer. Allows for further complexity.\n",
        "        state = F.relu(self.layer2(state))\n",
        "\n",
        "        # The output layer: outputs the raw values for each action directly,\n",
        "        # without applying any non-linearity (like softmax).\n",
        "        # These values can be interpreted as action preference scores in the\n",
        "        # Q-learning context.\n",
        "        # Compute value and advantage streams\n",
        "        #find the best value to choose from one of the networks with the aim of finding a value that is state dependant and is action independant\n",
        "        value = self.value_head(state)\n",
        "        #find how much better an action is compared to other value is compared with the finding from the value head network\n",
        "        advantage = self.advantage_head(state)\n",
        "\n",
        "        # Combine value and advantage to get final Q-values\n",
        "        #use the above values to find the Q value for this DQN, then we want to ensure that the advantage value isn't too large so that it doens't mess with the overall q values too much\n",
        "        q_values = value + (advantage - advantage.mean(dim = 1, keepdim = True))\n",
        "        return q_values\n",
        "\n",
        "\n",
        "#this replay buffer is here to ensure that the weights in teh network don't become too suspectible to sudden changes during training\n",
        "class PrioritizedReplayMemory(object):\n",
        "    \"\"\"\n",
        "    Crab&eacute;, G. (2020) How to implement prioritized experience replay for a deep Q-Network, Medium. Available at: https://towardsdatascience.com/how-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b (Accessed: 04 May 2024).\n",
        "    \"\"\"\n",
        "    def __init__(self, storage, alpha = 0.6):\n",
        "         # Create a new replay memory with the specified maximum storage\n",
        "        self.memory = deque([], maxlen =storage)\n",
        "        self.alpha = alpha\n",
        "        self.priorities = deque([], maxlen=storage)\n",
        "\n",
        "    def push(self, *args):\n",
        "        # Add a new transition to the memory. If memory is full, oldest entries are dropped\n",
        "        if self.priorities:\n",
        "            max_priority = max(self.priorities)\n",
        "        else:\n",
        "            max_priority = 1.0\n",
        "\n",
        "        self.memory.append(Transition(*args))\n",
        "        self.priorities.append(max_priority)\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # Randomly sample a batch of transitions from memory\n",
        "        priorities = np.array(self.priorities)\n",
        "\n",
        "        #determine the value of the current prioristies by applying the alpha value\n",
        "        priorities = priorities ** self.alpha\n",
        "\n",
        "        #determine the probability any given priority will be selected\n",
        "        probs = priorities / sum(priorities)\n",
        "        #randomly select a bunch of positions within the memory that is equalivant of the size of the batch\n",
        "        indices = np.random.choice(len(self.memory), batch_size, p=probs)\n",
        "        #get values out of the memory so that it can be a sample of the replay buffer that can be used for updating networks\n",
        "        samples = []\n",
        "        for indice in indices:\n",
        "            sample = self.memory[indice]\n",
        "            samples.append(sample)\n",
        "        return samples\n",
        "\n",
        "    def update_priorities(self, indices, priorities):\n",
        "        #update the priorities within the replay buffer --> this ensures that the most valuable experiences are kept\n",
        "        for indice, priority in zip(indices, priorities):\n",
        "            self.priorities[indice] = priority\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the current size of the internal memory\n",
        "        return len(self.memory)\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \"\"\"\n",
        "    Source for Base DQN Paszke, A. (2024) Reinforcement learning (DQN) tutorial, Reinforcement Learning (DQN) Tutorial - PyTorch Tutorials 2.3.0+cu121 documentation.\n",
        "    Available at: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html (Accessed: 04 May 2024).\n",
        "\n",
        "\n",
        "    Paszke, A. (2024) Reinforcement learning (DQN) tutorial, Reinforcement Learning (DQN) Tutorial - PyTorch Tutorials 2.3.0+cu121 documentation. Available at: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html (Accessed: 04 May 2024).\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        # Initializing training parameters\n",
        "        self.batch_size = 128 # Number of samples for each training batch\n",
        "        self.gamma = 0.99 # Discount factor for future rewards\n",
        "        self.epsilon_start_decay = 0.9  #Starting value of epsilon for the epsilon-greedy action selection\n",
        "        self.epsilon_end_decay = 0.05 # Final value of epsilon for the epsilon-greedy action selection\n",
        "        self.epsilon_decay_rate = 1000 # Rate of decay of epsilon\n",
        "        self.tau = 0.005 # Soft update parameter for target network\n",
        "        self.lr = 0.001  # Learning rate for the optimizer\n",
        "        self.num_episodes = 300\n",
        "        self.steps_done = 0 # Counter for total steps done\n",
        "        self.episode_durations = [] # List to store duration of each episode\n",
        "        self.episode_rewards = [] #keep count of the reward to avoid the graph issue we had when migrating to new environment\n",
        "        self.episode_lengths = [] #ditto\n",
        "\n",
        "    def setup_neural_networks(self):\n",
        "        # Initialize environment\n",
        "        self.env = env\n",
        "        n_actions = env.action_space.n # Get number of possible actions from environment\n",
        "        state = self.env.reset() # Reset environment to start state\n",
        "        #Change to this if the line above isn't working: state,_ = self.env.reset()\n",
        "        n_observations = len(state) # Get state dimension\n",
        "\n",
        "         # Initialize policy network and target network with the same architecture and weights\n",
        "        # part one of the psedudeo solution (apart of the class so it can be accessed everywhere\n",
        "        self.policy_net = DuelingDQN(n_observations, n_actions).to(device)\n",
        "        self.target_net = DuelingDQN(n_observations, n_actions).to(device)\n",
        "\n",
        "        # Copy weights from the policy network to target network so that they start exactly the same\n",
        "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
        "\n",
        "        #set the target network to the evulation mode so that it behaves as intended --> rather than being a bit fucky\n",
        "        self.target_net.eval()\n",
        "\n",
        "    def setup_replay_memory(self):\n",
        "        # Replay memory to store transitions\n",
        "        self.memory = PrioritizedReplayMemory(50000)  # Initialize replay memory with storage of 50,000\n",
        "\n",
        "    def setup_optimizer(self):\n",
        "        # Initialize optimizer for policy network\n",
        "        self.optimizer = optim.AdamW(self.policy_net.parameters(), lr = self.lr, amsgrad = True)\n",
        "\n",
        "    def select_action(self, state, eps):\n",
        "        # Generate a random sample for epsilon-greedy selection\n",
        "        # Start only using the q values (No more exploration)\n",
        "        if (eps >= 150):\n",
        "            with torch.no_grad():\n",
        "                q_values = self.policy_net(state)\n",
        "                return q_values.argmax(dim = 1, keepdim = True)\n",
        "        else:\n",
        "            # Find the dynamically changing epsilon threshold for epsilon-greedy strategy --> allows for the rate of exploration to decline and improve the agents performance\n",
        "            eps_threshold = self.epsilon_end_decay + (self.epsilon_start_decay - self.epsilon_end_decay) * math.exp(-1. * self.steps_done / self.epsilon_decay_rate)\n",
        "\n",
        "            # Increment the number of steps done so far\n",
        "            self.steps_done += 1\n",
        "\n",
        "            #this is the choice where the agent will exploit or explore\n",
        "            if random.random() > eps_threshold:\n",
        "                #exploit option\n",
        "                with torch.no_grad():\n",
        "                    #get the Q values and selext the next best option\n",
        "                    q_values = self.policy_net(state)\n",
        "                    return q_values.argmax(dim=1, keepdim=True)\n",
        "            else:\n",
        "                #exploration option\n",
        "                return torch.tensor([[self.env.action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n",
        "    def huber_loss_optimize_model(self):\n",
        "        if len(self.memory) < self.batch_size:\n",
        "            return # Exit if not enough samples\n",
        "\n",
        "        #get a sample of the memory so that a set of transitions can be further manipulated\n",
        "        transitions = self.memory.sample(self.batch_size)\n",
        "        #get the data ready for data manipulation\n",
        "        batch = Transition(*zip(*transitions))\n",
        "\n",
        "        #create a mask to filter the states that are the non final states leading up to the end of the episode\n",
        "        non_final_mask = []\n",
        "        for elementA in batch.next_state:\n",
        "          if elementA is not None:\n",
        "            non_final_mask.append(True)\n",
        "          else:\n",
        "            non_final_mask.append(False)\n",
        "        non_final_mask = torch.tensor(non_final_mask, device = device, dtype = torch.bool)\n",
        "\n",
        "        #further filter of the non final states based whether it lead to a none None value\n",
        "        non_final_next_states = []\n",
        "        for elementB in batch.next_state:\n",
        "          if elementB is not None:\n",
        "            non_final_next_states.append(elementB)\n",
        "        non_final_next_states = torch.cat(non_final_next_states)\n",
        "\n",
        "        #combine/concatenate the current states, the actions and the rewards for the episode\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "        #find the Q value for the current states and action taken from the policy network\n",
        "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "        #intialise the next state values\n",
        "        next_state_values = torch.zeros(self.batch_size, device = device)\n",
        "        #do not do the gradiant calcluations (this messed with prior iterations and is required code)\n",
        "        with torch.no_grad():\n",
        "            #predict actions for the next states using the policy network\n",
        "            next_state_actions = self.policy_net(non_final_next_states).argmax(1, keepdim=True)\n",
        "            #choose actions from the Q network from the target network\n",
        "            next_state_values[non_final_mask] = self.target_net(non_final_next_states).gather(1, next_state_actions).squeeze()\n",
        "\n",
        "        #Bellman equation for finding the expected Q values\n",
        "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
        "\n",
        "        # Find Huber loss value\n",
        "        loss = F.smooth_l1_loss(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        #reset the graidents within the optimiser so that training can occur as intended\n",
        "        self.optimizer.zero_grad()\n",
        "        #propergate the huber loss through the network --> who doesn't want to change their weight(s)\n",
        "        loss.backward()\n",
        "        #clip the gradiant value so avoid the chance that learning gets destabilised\n",
        "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
        "        #ensure that the graidants and the networks weights are updated\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def update_target_network(self):\n",
        "        #update target network's weights based on the policy network's weights\n",
        "        target_net_state_dict = self.target_net.state_dict()\n",
        "        policy_net_state_dict = self.policy_net.state_dict()\n",
        "        for key in policy_net_state_dict:\n",
        "\n",
        "            # Blend weights from policy network into target network\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key] * self.tau + target_net_state_dict[key] * (1 - self.tau)\n",
        "        self.target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "    def plot_durations(self, show_result=False):\n",
        "        plt.figure(2)\n",
        "        plt.clf()\n",
        "        rewards_t = torch.tensor(self.episode_rewards, dtype=torch.float)\n",
        "        if show_result:\n",
        "            plt.title('Result')\n",
        "        else:\n",
        "            plt.title('Training the Agent')\n",
        "        plt.xlabel('Episode')\n",
        "\n",
        "        plt.ylabel('Cumulative Reward')\n",
        "        plt.plot(rewards_t.numpy())\n",
        "        if len(rewards_t) >= 100:\n",
        "            means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "            means = torch.cat((torch.zeros(99), means))\n",
        "            plt.plot(means.numpy())\n",
        "\n",
        "        #prevents eye pain\n",
        "        plt.pause(0.001)\n",
        "        if is_ipython:\n",
        "            if not show_result:\n",
        "                display.display(plt.gcf())\n",
        "                display.clear_output(wait=True)\n",
        "            else:\n",
        "                display.display(plt.gcf())\n",
        "\n",
        "\n",
        "    def additional_graphs(self):\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.episode_rewards)\n",
        "        plt.title('Cumulative Reward per Episode')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Cumulative Reward')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.episode_lengths)\n",
        "        plt.title('Episode Length over Time')\n",
        "        plt.xlabel('Episode')\n",
        "        plt.ylabel('Length')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G. and Petersen, S., 2015. Human-level control through deep reinforcement learning. nature, 518(7540), pp.529-533.\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        Simonini, T., 2022. Deep Q-Learning With Space Invaders. Available at: https://huggingface.co/blog/deep-rl-dqn (Accessed 5 May 2024).\n",
        "        \"\"\"\n",
        "        #initialisation phase\n",
        "        self.setup_replay_memory()\n",
        "        self.setup_neural_networks()\n",
        "        self.setup_optimizer()\n",
        "        #self.env.start_video_recorder()\n",
        "\n",
        "        # Loop over each episode\n",
        "        for i_episode in range(self.num_episodes):\n",
        "            #Below is there to ensure that the recordings for the video is ther as intended\n",
        "            # Selective rendering\n",
        "            # NOTE: do not delete\n",
        "            #if i_episode % 50 == 0:\n",
        "                #self.env.start_video_recorder()\n",
        "            #if i_episode % 25 == 0:\n",
        "                #tmp_env = gym.make('LunarLander-v2', render_mode='human')\n",
        "                #self.env = gym.wrappers.RecordVideo(env=tmp_env, video_folder=\"/Desktop/video\", name_prefix=\"test-video\", episode_trigger=lambda x: x % 25 == 0)\n",
        "                #self.env = RecordVideo(self.env, video_folder=\"./videos\", episode_trigger=i_episode, disable_logger=True)\n",
        "\n",
        "           # else:\n",
        "                #self.env = gym.make('LunarLander-v2', render_mode=None)\n",
        "\n",
        "            #initialise the episode\n",
        "            cumulative_reward = 0\n",
        "            state = self.env.reset()\n",
        "            #Change to this if the line above isn't working: state,_ = self.env.reset()\n",
        "            state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "             # 1000 long loop for each step in the episode --> hard limit needed for the environment\n",
        "            for t in range(1000):\n",
        "                #first classic block similar to tabular Q learning:\n",
        "                action = self.select_action(state, i_episode) # Select an action based on the current state\n",
        "                observation, reward, done, _= self.env.step(action.item()) # Perform the action in the environment\n",
        "                # Change to this if the line above isn't working: observation, reward, done, _, info= self.env.step(action.item())\n",
        "\n",
        "                cumulative_reward += reward # Add the reward to the cumulative reward\n",
        "\n",
        "                # Convert reward to tensor and flag if it's the end of the episode\n",
        "                reward = torch.tensor([reward], device=device)\n",
        "                if done:\n",
        "                  next_state = None\n",
        "                else:\n",
        "                  next_state = torch.tensor(observation, dtype = torch.float32, device = device).unsqueeze(0)\n",
        "                #env.render()\n",
        "                #Second Big difference:\n",
        "                # Store the transition in the replay memory\n",
        "                self.memory.push(state, action, next_state, reward)\n",
        "\n",
        "                #update the target network\n",
        "                self.update_target_network()\n",
        "\n",
        "                # optimize the policy wiht Huber loss\n",
        "                self.huber_loss_optimize_model()\n",
        "\n",
        "                # Move to the next state to continue the learning process\n",
        "                state = next_state\n",
        "\n",
        "                # The episode has been completed\n",
        "                if done:\n",
        "                    self.episode_rewards.append(cumulative_reward)\n",
        "                    self.episode_lengths.append(t + 1)\n",
        "                    self.episode_durations.append(t + 1)\n",
        "                    self.plot_durations()\n",
        "                    break\n",
        "\n",
        "        print('Complete')\n",
        "        self.plot_durations(show_result=True) # Show final results as a plot\n",
        "        plt.ioff() # Turn off interactive plotting -- as i can be jarring\n",
        "        plt.show() # Show our amazing agent\n",
        "\n",
        "        #env.close_video_recorder()\n",
        "        #env.close()\n",
        "\n",
        "\n",
        "# Train the agent\n",
        "trainer = Trainer()\n",
        "trainer.train()\n",
        "trainer.additional_graphs()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3m45F7FZraz",
        "outputId": "24c2234f-7b67-4019-bdf7-bddc3340f932",
        "tags": []
      },
      "outputs": [
        {
          "ename": "SyntaxError",
          "evalue": "invalid syntax (1836190973.py, line 1)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[2], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    References:\u001b[0m\n\u001b[0m                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ],
      "source": [
        "#References:\n",
        "\n",
        "'''\n",
        "Source for Base DQN\n",
        "Paszke, A. (2024) Reinforcement learning (DQN) tutorial, Reinforcement Learning (DQN) Tutorial - PyTorch Tutorials 2.3.0+cu121 documentation. Available at: https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html (Accessed: 04 May 2024).\n",
        "'''\n",
        "'''\n",
        "Source for Dueling DQN\n",
        "Curt-Park (2024) Dueling Network, Jupyter Notebook Viewer. Available at: https://nbviewer.org/github/Curt-Park/rainbow-is-all-you-need/blob/master/04.dueling.ipynb (Accessed: 04 May 2024).\n",
        "'''\n",
        "'''\n",
        "Source for Prioritized Experience Replay\n",
        "Crab&eacute;, G. (2020) How to implement prioritized experience replay for a deep Q-Network, Medium. Available at: https://towardsdatascience.com/how-to-implement-prioritized-experience-replay-for-a-deep-q-network-a710beecd77b (Accessed: 04 May 2024).\n",
        "'''\n",
        "'''\n",
        "Source for Double DQN\n",
        "Curt-Park (2024) Double DQN, GitHub. Available at: https://github.com/Curt-Park/rainbow-is-all-you-need/blob/master/02.double_q.ipynb (Accessed: 04 May 2024).\n",
        "'''\n",
        "'''\n",
        "Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A.A., Veness, J., Bellemare, M.G., Graves, A., Riedmiller, M., Fidjeland, A.K., Ostrovski, G. and Petersen, S., 2015. Human-level control through deep reinforcement learning. nature, 518(7540), pp.529-533.\n",
        "'''\n",
        "\"\"\"\n",
        "Simonini, T., 2022. Deep Q-Learning With Space Invaders. Available at: https://huggingface.co/blog/deep-rl-dqn (Accessed 5 May 2024).\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}